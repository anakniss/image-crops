{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "51216106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6183cf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ff5b4bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASSES = [\n",
    "    'almond', 'banana', 'cardamom', 'Cherry', 'chilli', 'clove', 'coconut',\n",
    "    'Coffee-plant', 'cotton', 'Cucumber', 'Fox_nut(Makhana)', 'gram', 'jowar',\n",
    "    'jute', 'Lemon', 'maize', 'mustard-oil', 'Olive-tree', 'papaya',\n",
    "    'Pearl_millet(bajra)', 'pineapple', 'rice', 'soyabean', 'sugarcane',\n",
    "    'sunflower', 'tea', 'Tobacco-plant', 'tomato', 'vigna-radiati(Mung)', 'wheat'\n",
    "]\n",
    "\n",
    "\n",
    "CLASSES_PATH = \"C:\\\\Code\\\\image-crops\\\\Cops_DB\\\\Agricultural-crops\"\n",
    "IMG_SIZE = (128, 128)\n",
    "TRAIN_IMAGES = 24\n",
    "VAL_IMAGES = 8         \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "be96c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    X_train, y_train = [], []\n",
    "    X_val, y_val = [], []\n",
    "\n",
    "    print(\"Carregando imagens...\\n\")\n",
    "\n",
    "    for class_idx, class_name in enumerate(CLASSES):\n",
    "        class_path = os.path.join(CLASSES_PATH, class_name)\n",
    "\n",
    "        if not os.path.exists(class_path):\n",
    "            print(f\"Aviso: Pasta {class_name} não encontrada!\")\n",
    "            continue\n",
    "\n",
    "        image_files = [f for f in os.listdir(class_path)\n",
    "                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:32]\n",
    "\n",
    "        train_files = image_files[:TRAIN_IMAGES]\n",
    "        val_files = image_files[TRAIN_IMAGES:32]\n",
    "\n",
    "        for img_file in train_files:\n",
    "            try:\n",
    "                img = Image.open(os.path.join(class_path, img_file)).convert('RGB')\n",
    "                img = img.resize(IMG_SIZE)\n",
    "                img_array = np.array(img)/255.0\n",
    "                img_array = np.transpose(img_array,(2,0,1))\n",
    "                X_train.append(img_array)\n",
    "                y_train.append(class_idx)\n",
    "            except Exception as e:\n",
    "                print(\"Erro:\", e)\n",
    "\n",
    "        for img_file in val_files:\n",
    "            try:\n",
    "                img = Image.open(os.path.join(class_path, img_file)).convert('RGB')\n",
    "                img = img.resize(IMG_SIZE)\n",
    "                img_array = np.array(img)/255.0\n",
    "                img_array = np.transpose(img_array,(2,0,1))\n",
    "                X_val.append(img_array)\n",
    "                y_val.append(class_idx)\n",
    "            except Exception as e:\n",
    "                print(\"Erro:\", e)\n",
    "\n",
    "        print(f\"Classe {class_name}: Treino {len(train_files)}, Validação {len(val_files)}\")\n",
    "\n",
    "    X_train = torch.tensor(np.array(X_train), dtype=torch.float32)\n",
    "    y_train = torch.tensor(np.array(y_train), dtype=torch.long)\n",
    "    X_val = torch.tensor(np.array(X_val), dtype=torch.float32)\n",
    "    y_val = torch.tensor(np.array(y_val), dtype=torch.long)\n",
    "\n",
    "    print(\"\\nDataset final:\")\n",
    "    print(\"Treino:\", X_train.shape, \"| Val:\", X_val.shape)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "18779c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando imagens...\n",
      "\n",
      "Classe almond: Treino 21, Validação 0\n",
      "Classe banana: Treino 24, Validação 7\n",
      "Classe cardamom: Treino 22, Validação 0\n",
      "Classe Cherry: Treino 24, Validação 8\n",
      "Classe chilli: Treino 23, Validação 0\n",
      "Classe clove: Treino 24, Validação 6\n",
      "Classe coconut: Treino 24, Validação 1\n",
      "Classe Coffee-plant: Treino 24, Validação 5\n",
      "Classe cotton: Treino 24, Validação 8\n",
      "Classe Cucumber: Treino 24, Validação 7\n",
      "Classe Fox_nut(Makhana): Treino 23, Validação 0\n",
      "Classe gram: Treino 24, Validação 1\n",
      "Classe jowar: Treino 24, Validação 6\n",
      "Classe jute: Treino 23, Validação 0\n",
      "Classe Lemon: Treino 24, Validação 4\n",
      "Classe maize: Treino 24, Validação 7\n",
      "Classe mustard-oil: Treino 24, Validação 4\n",
      "Classe Olive-tree: Treino 24, Validação 6\n",
      "Classe papaya: Treino 23, Validação 0\n",
      "Classe Pearl_millet(bajra): Treino 24, Validação 8\n",
      "Classe pineapple: Treino 24, Validação 1\n",
      "Classe rice: Treino 24, Validação 5\n",
      "Classe soyabean: Treino 24, Validação 6\n",
      "Classe sugarcane: Treino 24, Validação 1\n",
      "Classe sunflower: Treino 24, Validação 0\n",
      "Classe tea: Treino 23, Validação 0\n",
      "Classe Tobacco-plant: Treino 24, Validação 8\n",
      "Classe tomato: Treino 24, Validação 2\n",
      "Classe vigna-radiati(Mung): Treino 24, Validação 3\n",
      "Classe wheat: Treino 24, Validação 7\n",
      "\n",
      "Dataset final:\n",
      "Treino: torch.Size([710, 3, 128, 128]) | Val: torch.Size([111, 3, 128, 128])\n",
      "Loaders prontos!\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = load_and_preprocess_data()\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"Loaders prontos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "faa2405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),  # 128 -> 64\n",
    "\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),  # 64 -> 32\n",
    "\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),  # 32 -> 16\n",
    "\n",
    "        nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),  # 16 -> 8\n",
    "\n",
    "        nn.Flatten(),\n",
    "\n",
    "        nn.Linear(256 * 8 * 8, 512),  # ALTERADO!\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cf57da39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU()\n",
       "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU()\n",
       "  (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (16): Flatten(start_dim=1, end_dim=-1)\n",
       "  (17): Linear(in_features=16384, out_features=512, bias=True)\n",
       "  (18): ReLU()\n",
       "  (19): Dropout(p=0.5, inplace=False)\n",
       "  (20): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (21): ReLU()\n",
       "  (22): Dropout(p=0.5, inplace=False)\n",
       "  (23): Linear(in_features=256, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(CLASSES)\n",
    "\n",
    "model = create_model(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "444c70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    _, preds_max = torch.max(preds, 1)\n",
    "    return (preds_max == labels).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "13b4a1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando Epoch 1/100: 100%|██████████| 45/45 [00:01<00:00, 27.31it/s]\n",
      "Treinando Epoch 2/100: 100%|██████████| 45/45 [00:01<00:00, 29.42it/s]\n",
      "Treinando Epoch 3/100: 100%|██████████| 45/45 [00:01<00:00, 30.44it/s]\n",
      "Treinando Epoch 4/100: 100%|██████████| 45/45 [00:01<00:00, 30.44it/s]\n",
      "Treinando Epoch 5/100: 100%|██████████| 45/45 [00:01<00:00, 30.40it/s]\n",
      "Treinando Epoch 6/100: 100%|██████████| 45/45 [00:01<00:00, 30.40it/s]\n",
      "Treinando Epoch 7/100: 100%|██████████| 45/45 [00:01<00:00, 30.42it/s]\n",
      "Treinando Epoch 8/100: 100%|██████████| 45/45 [00:01<00:00, 30.53it/s]\n",
      "Treinando Epoch 9/100: 100%|██████████| 45/45 [00:01<00:00, 29.91it/s]\n",
      "Treinando Epoch 10/100: 100%|██████████| 45/45 [00:01<00:00, 29.86it/s]\n",
      "Treinando Epoch 11/100: 100%|██████████| 45/45 [00:01<00:00, 30.40it/s]\n",
      "Treinando Epoch 12/100: 100%|██████████| 45/45 [00:01<00:00, 30.49it/s]\n",
      "Treinando Epoch 13/100: 100%|██████████| 45/45 [00:01<00:00, 30.91it/s]\n",
      "Treinando Epoch 14/100: 100%|██████████| 45/45 [00:01<00:00, 30.78it/s]\n",
      "Treinando Epoch 15/100: 100%|██████████| 45/45 [00:01<00:00, 30.91it/s]\n",
      "Treinando Epoch 16/100: 100%|██████████| 45/45 [00:01<00:00, 29.26it/s]\n",
      "Treinando Epoch 17/100: 100%|██████████| 45/45 [00:01<00:00, 30.54it/s]\n",
      "Treinando Epoch 18/100: 100%|██████████| 45/45 [00:01<00:00, 30.36it/s]\n",
      "Treinando Epoch 19/100: 100%|██████████| 45/45 [00:01<00:00, 30.21it/s]\n",
      "Treinando Epoch 20/100: 100%|██████████| 45/45 [00:01<00:00, 30.73it/s]\n",
      "Treinando Epoch 21/100: 100%|██████████| 45/45 [00:01<00:00, 30.77it/s]\n",
      "Treinando Epoch 22/100: 100%|██████████| 45/45 [00:01<00:00, 30.75it/s]\n",
      "Treinando Epoch 23/100: 100%|██████████| 45/45 [00:01<00:00, 30.53it/s]\n",
      "Treinando Epoch 24/100: 100%|██████████| 45/45 [00:01<00:00, 30.41it/s]\n",
      "Treinando Epoch 25/100: 100%|██████████| 45/45 [00:01<00:00, 30.39it/s]\n",
      "Treinando Epoch 26/100: 100%|██████████| 45/45 [00:01<00:00, 30.41it/s]\n",
      "Treinando Epoch 27/100: 100%|██████████| 45/45 [00:01<00:00, 30.16it/s]\n",
      "Treinando Epoch 28/100: 100%|██████████| 45/45 [00:01<00:00, 30.43it/s]\n",
      "Treinando Epoch 29/100: 100%|██████████| 45/45 [00:01<00:00, 29.44it/s]\n",
      "Treinando Epoch 30/100: 100%|██████████| 45/45 [00:01<00:00, 30.80it/s]\n",
      "Treinando Epoch 31/100: 100%|██████████| 45/45 [00:01<00:00, 30.75it/s]\n",
      "Treinando Epoch 32/100: 100%|██████████| 45/45 [00:01<00:00, 30.39it/s]\n",
      "Treinando Epoch 33/100: 100%|██████████| 45/45 [00:01<00:00, 30.64it/s]\n",
      "Treinando Epoch 34/100: 100%|██████████| 45/45 [00:01<00:00, 30.49it/s]\n",
      "Treinando Epoch 35/100: 100%|██████████| 45/45 [00:01<00:00, 29.89it/s]\n",
      "Treinando Epoch 36/100: 100%|██████████| 45/45 [00:01<00:00, 30.45it/s]\n",
      "Treinando Epoch 37/100: 100%|██████████| 45/45 [00:01<00:00, 30.14it/s]\n",
      "Treinando Epoch 38/100: 100%|██████████| 45/45 [00:01<00:00, 29.43it/s]\n",
      "Treinando Epoch 39/100: 100%|██████████| 45/45 [00:01<00:00, 30.38it/s]\n",
      "Treinando Epoch 40/100: 100%|██████████| 45/45 [00:01<00:00, 30.07it/s]\n",
      "Treinando Epoch 41/100: 100%|██████████| 45/45 [00:01<00:00, 30.45it/s]\n",
      "Treinando Epoch 42/100: 100%|██████████| 45/45 [00:01<00:00, 30.44it/s]\n",
      "Treinando Epoch 43/100: 100%|██████████| 45/45 [00:01<00:00, 29.06it/s]\n",
      "Treinando Epoch 44/100: 100%|██████████| 45/45 [00:01<00:00, 30.22it/s]\n",
      "Treinando Epoch 45/100: 100%|██████████| 45/45 [00:01<00:00, 30.22it/s]\n",
      "Treinando Epoch 46/100: 100%|██████████| 45/45 [00:01<00:00, 30.55it/s]\n",
      "Treinando Epoch 47/100: 100%|██████████| 45/45 [00:01<00:00, 30.87it/s]\n",
      "Treinando Epoch 48/100: 100%|██████████| 45/45 [00:01<00:00, 30.72it/s]\n",
      "Treinando Epoch 49/100: 100%|██████████| 45/45 [00:01<00:00, 30.82it/s]\n",
      "Treinando Epoch 50/100: 100%|██████████| 45/45 [00:01<00:00, 29.40it/s]\n",
      "Treinando Epoch 51/100: 100%|██████████| 45/45 [00:01<00:00, 30.22it/s]\n",
      "Treinando Epoch 52/100: 100%|██████████| 45/45 [00:01<00:00, 29.68it/s]\n",
      "Treinando Epoch 53/100: 100%|██████████| 45/45 [00:01<00:00, 30.46it/s]\n",
      "Treinando Epoch 54/100: 100%|██████████| 45/45 [00:01<00:00, 30.36it/s]\n",
      "Treinando Epoch 55/100: 100%|██████████| 45/45 [00:01<00:00, 30.46it/s]\n",
      "Treinando Epoch 56/100: 100%|██████████| 45/45 [00:01<00:00, 30.71it/s]\n",
      "Treinando Epoch 57/100: 100%|██████████| 45/45 [00:01<00:00, 30.18it/s]\n",
      "Treinando Epoch 58/100: 100%|██████████| 45/45 [00:01<00:00, 29.69it/s]\n",
      "Treinando Epoch 59/100: 100%|██████████| 45/45 [00:01<00:00, 30.16it/s]\n",
      "Treinando Epoch 60/100: 100%|██████████| 45/45 [00:01<00:00, 30.35it/s]\n",
      "Treinando Epoch 61/100: 100%|██████████| 45/45 [00:01<00:00, 30.19it/s]\n",
      "Treinando Epoch 62/100: 100%|██████████| 45/45 [00:01<00:00, 27.39it/s]\n",
      "Treinando Epoch 63/100: 100%|██████████| 45/45 [00:01<00:00, 30.36it/s]\n",
      "Treinando Epoch 64/100: 100%|██████████| 45/45 [00:01<00:00, 30.40it/s]\n",
      "Treinando Epoch 65/100: 100%|██████████| 45/45 [00:01<00:00, 30.09it/s]\n",
      "Treinando Epoch 66/100: 100%|██████████| 45/45 [00:01<00:00, 30.06it/s]\n",
      "Treinando Epoch 67/100: 100%|██████████| 45/45 [00:01<00:00, 29.73it/s]\n",
      "Treinando Epoch 68/100: 100%|██████████| 45/45 [00:01<00:00, 30.18it/s]\n",
      "Treinando Epoch 69/100: 100%|██████████| 45/45 [00:01<00:00, 30.14it/s]\n",
      "Treinando Epoch 70/100: 100%|██████████| 45/45 [00:01<00:00, 29.91it/s]\n",
      "Treinando Epoch 71/100: 100%|██████████| 45/45 [00:01<00:00, 30.27it/s]\n",
      "Treinando Epoch 72/100: 100%|██████████| 45/45 [00:01<00:00, 27.78it/s]\n",
      "Treinando Epoch 73/100: 100%|██████████| 45/45 [00:01<00:00, 30.32it/s]\n",
      "Treinando Epoch 74/100: 100%|██████████| 45/45 [00:01<00:00, 30.34it/s]\n",
      "Treinando Epoch 75/100: 100%|██████████| 45/45 [00:01<00:00, 30.23it/s]\n",
      "Treinando Epoch 76/100: 100%|██████████| 45/45 [00:01<00:00, 30.44it/s]\n",
      "Treinando Epoch 77/100: 100%|██████████| 45/45 [00:01<00:00, 29.85it/s]\n",
      "Treinando Epoch 78/100: 100%|██████████| 45/45 [00:01<00:00, 30.34it/s]\n",
      "Treinando Epoch 79/100: 100%|██████████| 45/45 [00:01<00:00, 30.02it/s]\n",
      "Treinando Epoch 80/100: 100%|██████████| 45/45 [00:01<00:00, 30.17it/s]\n",
      "Treinando Epoch 81/100: 100%|██████████| 45/45 [00:01<00:00, 30.36it/s]\n",
      "Treinando Epoch 82/100: 100%|██████████| 45/45 [00:01<00:00, 30.15it/s]\n",
      "Treinando Epoch 83/100: 100%|██████████| 45/45 [00:01<00:00, 29.08it/s]\n",
      "Treinando Epoch 84/100: 100%|██████████| 45/45 [00:01<00:00, 30.03it/s]\n",
      "Treinando Epoch 85/100: 100%|██████████| 45/45 [00:01<00:00, 30.27it/s]\n",
      "Treinando Epoch 86/100: 100%|██████████| 45/45 [00:01<00:00, 30.40it/s]\n",
      "Treinando Epoch 87/100: 100%|██████████| 45/45 [00:01<00:00, 30.04it/s]\n",
      "Treinando Epoch 88/100: 100%|██████████| 45/45 [00:01<00:00, 30.16it/s]\n",
      "Treinando Epoch 89/100: 100%|██████████| 45/45 [00:01<00:00, 29.80it/s]\n",
      "Treinando Epoch 90/100: 100%|██████████| 45/45 [00:01<00:00, 28.64it/s]\n",
      "Treinando Epoch 91/100: 100%|██████████| 45/45 [00:01<00:00, 30.33it/s]\n",
      "Treinando Epoch 92/100: 100%|██████████| 45/45 [00:01<00:00, 30.10it/s]\n",
      "Treinando Epoch 93/100: 100%|██████████| 45/45 [00:01<00:00, 28.85it/s]\n",
      "Treinando Epoch 94/100: 100%|██████████| 45/45 [00:01<00:00, 30.42it/s]\n",
      "Treinando Epoch 95/100: 100%|██████████| 45/45 [00:01<00:00, 30.41it/s]\n",
      "Treinando Epoch 96/100: 100%|██████████| 45/45 [00:01<00:00, 30.45it/s]\n",
      "Treinando Epoch 97/100: 100%|██████████| 45/45 [00:01<00:00, 30.42it/s]\n",
      "Treinando Epoch 98/100: 100%|██████████| 45/45 [00:01<00:00, 30.42it/s]\n",
      "Treinando Epoch 99/100: 100%|██████████| 45/45 [00:01<00:00, 30.48it/s]\n",
      "Treinando Epoch 100/100: 100%|██████████| 45/45 [00:01<00:00, 30.01it/s]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Treinando Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_acc += accuracy(outputs, labels)\n",
    "\n",
    "    # print(f\"\\nEpoch {epoch+1}: Loss={running_loss/len(train_loader):.4f}, \"\n",
    "    #       f\"Acc={running_acc/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0ad02b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validação -> Loss=2.7240, Accuracy=0.2268\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "val_loss = 0\n",
    "val_acc = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        val_acc += accuracy(outputs, labels)\n",
    "\n",
    "print(f\"\\nValidação -> Loss={val_loss/len(val_loader):.4f}, Accuracy={val_acc/len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0c1cad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_single_image(path):\n",
    "#     img = Image.open(path).convert(\"RGB\")\n",
    "#     img = img.resize(IMG_SIZE)\n",
    "#     img = np.array(img)/255.0\n",
    "#     img = np.transpose(img, (2,0,1))\n",
    "#     img = torch.tensor(img, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         output = model(img)\n",
    "#         pred = torch.argmax(output)\n",
    "\n",
    "#     return CLASSES[pred.item()]\n",
    "\n",
    "# predict_single_image(\"exemplo.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
